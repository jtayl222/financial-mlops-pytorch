{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba3616b-babe-4b59-a550-10b21bd3a579",
   "metadata": {},
   "source": [
    "### Why Version Control Notebooks (with caveats)?\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "1.  **Reproducibility (Code + Output):** The primary benefit is having a historical record of not just the code, but also the order of execution and the results (plots, tables, etc.). This can be invaluable for understanding past experiments or presentations.\n",
    "2.  **Easier Sharing:** Colleagues can pull the notebook and see the exact state you left it in, including intermediate results.\n",
    "3.  **Historical Record:** It tracks the evolution of your analysis and experimentation.\n",
    "\n",
    "**Cons (and why caveats are needed):**\n",
    "\n",
    "1.  **Terrible Diffing:** Jupyter notebooks are JSON files. Git diffs on JSON are notoriously unreadable, making it very hard to see what code changes were made, especially if outputs are large.\n",
    "2.  **Merge Conflicts:** Resolving merge conflicts in JSON can be a nightmare, often leading to manual clean-up or corrupted notebooks.\n",
    "3.  **Large File Sizes:** Storing outputs (especially images or large data frames) directly in the notebook can bloat the repository size.\n",
    "4.  **Sensitive Data Exposure:** If you accidentally print sensitive information (like API keys or customer data) to the output, it gets stored in the notebook's JSON and committed to Git.\n",
    "\n",
    "### Best Practices for a Small Data Science Team\n",
    "\n",
    "To mitigate the cons while retaining the pros, adopt a \"hybrid\" approach:\n",
    "\n",
    "1.  **Modularize Your Code (Most Important\\!):**\n",
    "\n",
    "      * **Extract Core Logic:** Any reusable code (like your `data_ingestion.py`, `data_processing.py`, `model_training.py`, `evaluation.py` etc.) should live in separate `.py` scripts. These are standard Python files that Git can diff and merge efficiently.\n",
    "      * **Notebooks as Orchestrators/Explorers/Reports:** Use notebooks for:\n",
    "          * Initial data exploration (EDA).\n",
    "          * Prototyping new features or model ideas.\n",
    "          * Running experiments by calling functions from your `.py` scripts.\n",
    "          * Visualizing results and telling the story of your analysis (like a living report).\n",
    "          * The actual \"main\" script that runs your pipeline should likely be a `.py` file, not a notebook.\n",
    "\n",
    "2.  **Clear Notebook Policy for Commits:**\n",
    "\n",
    "      * **Clear Outputs Before Committing:** This is the *most crucial* step. Use `Cell -> All Output -> Clear` in Jupyter before saving and committing. This significantly reduces file size, makes diffs cleaner, and prevents accidental sensitive data leaks.\n",
    "      * **Automate Clearing:**\n",
    "          * **`nbstripout`:** This is a fantastic tool that automatically strips outputs from notebooks before they are committed. Install it (`pip install nbstripout`) and then enable it as a Git filter:\n",
    "            ```bash\n",
    "            nbstripout --install\n",
    "            # Or for a single repo:\n",
    "            # cd your_repo_root\n",
    "            # nbstripout --install --force\n",
    "            ```\n",
    "            This means you can save your notebook with outputs for your own use, and Git will automatically strip them when you `git add` and `git commit`. Your collaborators will pull clean notebooks.\n",
    "          * **Git Hooks:** You can set up pre-commit hooks to run `nbstripout` or other cleaning scripts.\n",
    "\n",
    "3.  **Leverage Experiment Tracking (like MLflow):**\n",
    "\n",
    "      * You're already using MLflow\\! This is perfect. Don't rely on notebook outputs for tracking metrics, parameters, and models. Log everything important to MLflow. This separates the \"results\" from the \"code\" and makes your notebooks lighter.\n",
    "\n",
    "4.  **Adopt a Naming Convention:**\n",
    "\n",
    "      * Prefix notebooks based on their purpose (e.g., `01_EDA_initial_data.ipynb`, `02_Feature_Engineering_V1.ipynb`, `03_Model_Experiment_XGBoost.ipynb`).\n",
    "      * Consider adding author initials or dates if multiple people are working on similar topics simultaneously, but try to avoid parallel work on the *exact same* notebook.\n",
    "\n",
    "5.  **Small Team Advantages:**\n",
    "\n",
    "      * **Easier Communication:** With a small team, it's easier to establish and enforce these conventions early.\n",
    "      * **Less Overhead:** Tools like `nbstripout` are simple to set up and maintain for a small group.\n",
    "      * **Agility:** A clean codebase with modularized scripts and focused notebooks makes iteration faster.\n",
    "\n",
    "### In Summary:\n",
    "\n",
    "For your current situation, after successfully ingesting data:\n",
    "\n",
    "  * **Save your notebook.**\n",
    "  * **Clear all outputs** before committing.\n",
    "  * **Commit the cleaned notebook** to your Git repo.\n",
    "  * **Most importantly, consider how the data ingestion logic could be refactored into your `data_ingestion.py` file** if it's more than just a one-off execution. Your notebook would then just call the function from that script.\n",
    "\n",
    "By following these practices, your small data science team can effectively collaborate, maintain a clean and reproducible codebase, and avoid the common pitfalls of version controlling Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bcaea7-5f26-4259-9183-a7c95cdf635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd # Import pandas, as the downloaded data is a DataFrame\n",
    "\n",
    "# Assumes you are in the root of the financial-mlops-pytorch repo\n",
    "sys.path.append(os.path.abspath('financial-mlops-pytorch/src'))\n",
    "\n",
    "# Import the module AFTER sys.path is updated and kernel is restarted (if needed)\n",
    "import data_ingestion\n",
    "print(\"Successfully imported data_ingestion module.\")\n",
    "\n",
    "# --- Next Step: Download some data ---\n",
    "ticker = \"AAPL\" # Example: Apple Inc.\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "# Define the output directory within your shared storage\n",
    "output_base_dir = \"/mnt/shared-data\"\n",
    "output_dir_for_ticker = os.path.join(output_base_dir, ticker.lower())\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir_for_ticker, exist_ok=True)\n",
    "print(f\"Ensured output directory exists: {output_dir_for_ticker}\")\n",
    "\n",
    "# Determine the *expected* file path where the data will be saved by the download function\n",
    "# (The download function saves it, but returns the DataFrame)\n",
    "expected_file_name = f\"{ticker.upper()}_stock_data.csv\"\n",
    "expected_file_path = os.path.join(output_dir_for_ticker, expected_file_name)\n",
    "\n",
    "\n",
    "# Call the function to download data.\n",
    "# IMPORTANT: data_ingestion.download_stock_data returns the DataFrame directly,\n",
    "# and it also handles saving the data to the specified output_dir.\n",
    "print(f\"Downloading data for {ticker} from {start_date} to {end_date}...\")\n",
    "df_downloaded = data_ingestion.download_stock_data(ticker, start_date, end_date)\n",
    "\n",
    "print(f\"Data downloaded and available in Python DataFrame 'df_downloaded'.\")\n",
    "print(f\"Data should also be saved to: {expected_file_path}\") # This is the file that was saved to disk.\n",
    "\n",
    "\n",
    "# --- Inspect the downloaded DataFrame ---\n",
    "print(\"\\nFirst 5 rows of the downloaded data (from DataFrame in memory):\")\n",
    "print(df_downloaded.head())\n",
    "print(f\"\\nShape of the data: {df_downloaded.shape}\")\n",
    "\n",
    "# --- Optional: Verify the file exists on disk ---\n",
    "# You can check if the file was indeed saved as expected\n",
    "if os.path.exists(expected_file_path):\n",
    "    print(f\"\\nConfirmed: Data file '{expected_file_name}' exists on disk at {output_dir_for_ticker}/\")\n",
    "    # If you wanted to load it *again* from disk (e.g., in a new session), you'd use load_data like this:\n",
    "    # df_loaded_from_disk = data_ingestion.load_data(expected_file_path)\n",
    "    # print(\"\\nFirst 5 rows of data loaded from disk (for verification):\")\n",
    "    # print(df_loaded_from_disk.head())\n",
    "else:\n",
    "    print(f\"\\nWarning: Data file '{expected_file_name}' was NOT found on disk at {output_dir_for_ticker}/. Check download_stock_data implementation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
